# HierRetro: Hierarchical Retrosynthesis Prediction Framework

A PyTorch implementation of hierarchical retrosynthesis prediction combining GNN and Mamba (SSM) encoders with interpretable prediction heads.

## Features

- **Dual Encoder Architecture**: Choose between GNN (Uni-Mol+ style) or Mamba (State Space Model)
- **Hierarchical Prediction**: RC type â†’ RC localization â†’ Actions â†’ Termination
- **Dynamic Adaptive Multi-Task Learning (DAMT)**: Automatically balances task weights during training
- **Interpretable Pipeline**: Each decision step is explicit and traceable
- **CPU-Optimized Inference**: Efficient deployment on standard hardware

## Architecture Overview

```
Product Molecule
       â†“
   Encoder (GNN/Mamba)
       â†“
  â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”
  â”‚ RC Type â”‚  (Atom vs Bond)
  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
       â†“
  â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”
  â”‚ RC Loc. â”‚  (Which atoms/bonds)
  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
       â†“
  â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”
  â”‚ Actions â”‚  (H-changes, leaving groups, etc.)
  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
       â†“
  â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”
  â”‚  Term.  â”‚  (Stop or continue)
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Installation

### 1. Create Environment

```bash
# Create virtual environment
python -m venv venv

# Activate (Windows)
venv\Scripts\activate

# Activate (Linux/Mac)
source venv/bin/activate
```

### 2. Install Dependencies

```bash
pip install -r requirements.txt
```

**Note**: For Mamba encoder, install `mamba-ssm`:
```bash
pip install mamba-ssm causal-conv1d>=1.0.0
```

For GPU support, ensure PyTorch is installed with CUDA:
```bash
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
```

## Quick Start

### 1. Preprocess Data

```bash
python data/preprocess_uspto.py
```

This will:
- Load USPTO-50K dataset
- Canonicalize SMILES
- Extract reaction centers
- Build action vocabularies
- Create train/val/test splits

### 2. Train Model

**GNN Encoder:**
```bash
python training/train.py --config configs/gnn_config.yaml
```

**Mamba Encoder:**
```bash
python training/train.py --config configs/mamba_config.yaml
```

### 3. Inference

```python
from models import HierRetro
from inference.beam_search import BeamSearchRetrosynthesis
import torch

# Load model
model = HierRetro(encoder_type='gnn')
checkpoint = torch.load('checkpoints/best_model.pt')
model.load_state_dict(checkpoint['model_state_dict'])

# Predict reactants
searcher = BeamSearchRetrosynthesis(model, beam_width=10)
results = searcher.predict('CCO')  # Product SMILES

print(f"Predicted reactants: {results[0]['reactants']}")
print(f"Score: {results[0]['score']:.4f}")
```

## Configuration

Edit `configs/gnn_config.yaml` or `configs/mamba_config.yaml`:

```yaml
model:
  encoder_type: "gnn"  # or "mamba"
  hidden_dim: 256
  num_layers: 6

training:
  batch_size: 32
  learning_rate: 0.0001
  max_epochs: 100
  
  damt_loss:
    queue_size: 50
    tau: 1.0

data:
  dataset: "uspto_50k"
  train_split: 0.8
```

## Project Structure

```
retrosynthesis/
â”œâ”€â”€ data/                      # Data processing
â”‚   â”œâ”€â”€ preprocess_uspto.py   # USPTO-50K preprocessing
â”‚   â””â”€â”€ dataset.py            # PyTorch Dataset
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ encoders/             # Molecular encoders
â”‚   â”‚   â”œâ”€â”€ base_encoder.py  # Abstract base
â”‚   â”‚   â”œâ”€â”€ gnn_encoder.py   # GNN (Uni-Mol+)
â”‚   â”‚   â”œâ”€â”€ mamba_encoder.py # Mamba SSM
â”‚   â”‚   â””â”€â”€ graph_linearizer.py
â”‚   â”œâ”€â”€ prediction_heads/     # Task-specific heads
â”‚   â”‚   â”œâ”€â”€ rc_type_predictor.py
â”‚   â”‚   â”œâ”€â”€ atom_center_predictor.py
â”‚   â”‚   â”œâ”€â”€ bond_center_predictor.py
â”‚   â”‚   â”œâ”€â”€ action_predictor.py
â”‚   â”‚   â””â”€â”€ termination_predictor.py
â”‚   â””â”€â”€ hierretro.py          # Main model
â”œâ”€â”€ training/
â”‚   â”œâ”€â”€ train.py              # Training script
â”‚   â””â”€â”€ damt_loss.py          # Adaptive loss
â”œâ”€â”€ inference/
â”‚   â”œâ”€â”€ beam_search.py        # Beam search
â”‚   â””â”€â”€ explainability.py     # Visualization
â”œâ”€â”€ evaluation/
â”‚   â””â”€â”€ metrics.py            # Evaluation metrics
â”œâ”€â”€ configs/                   # YAML configs
â”‚   â”œâ”€â”€ gnn_config.yaml
â”‚   â””â”€â”€ mamba_config.yaml
â””â”€â”€ requirements.txt
```

## Key Features

### 1. Dynamic Adaptive Multi-Task Learning (DAMT)

Automatically balances task weights based on descent rates:
- Fast-descending tasks get lower weight
- Slow-descending tasks get higher weight
- Prevents any single task from dominating

### 2. Dual Encoder Support

**GNN (Uni-Mol+):**
- Pair-wise attention with bond features
- 6 transformer blocks
- 256-dim atom features
- ~10M parameters

**Mamba (SSM):**
- State-space sequential modeling
- O(n) complexity vs O(nÂ²) for transformers
- Graph linearization (DFS/BFS)
- 12 layers, 768-dim
- ~20M parameters

### 3. Hierarchical Interpretability

Each prediction step is explicit:
1. RC Type: "Is this an atom or bond center?"
2. RC Localization: "Which atom/bond specifically?"
3. Action: "What chemical change occurs?"
4. Termination: "Is retrosynthesis complete?"

## Evaluation

```python
from evaluation.metrics import compute_exact_match_accuracy, MetricsTracker

tracker = MetricsTracker()

# During evaluation
tracker.update('exact_match_acc', accuracy)
tracker.update('rc_type_acc', rc_accuracy)

# Get results
results = tracker.get_all_averages()
print(f"Top-1 Accuracy: {results['exact_match_acc']:.2%}")
```

## Performance Targets

Based on HierRetro paper (USPTO-50K, unknown reaction type):

| Metric | Target | GNN | Mamba |
|--------|--------|-----|-------|
| Top-1 Accuracy | 52.3% | TBD | TBD |
| Top-5 Accuracy | 83.1% | TBD | TBD |
| Top-10 Accuracy | 89.1% | TBD | TBD |
| RC ID (Top-1) | 72.5% | TBD | TBD |
| Round-trip (Top-5) | 96.2% | TBD | TBD |

## GNN vs Mamba Comparison

| Aspect | GNN | Mamba |
|--------|-----|-------|
| Complexity | O(nÂ²) | O(n) |
| Parameters | ~10M | ~20M |
| Training Time | Faster | Slower |
| Inference Speed (CPU) | Medium | **Fast** |
| Long Molecules (>50 atoms) | Slower | **2x Faster** |
| Interpretability | High | Medium |

## Citation

If you use this code, please cite:

```bibtex
@article{hierretro2024,
  title={HierRetro: Hierarchical Retrosynthesis Prediction},
  journal={arXiv preprint},
  year={2024}
}
```

## References

- **HierRetro**: Liu et al., 2024 (arXiv:2411.19503)
- **Uni-Mol+**: Zhou et al., 2023
- **Mamba**: Gu & Dao, 2023
- **USPTO-50K**: Coley et al., 2017

## Contributing

Contributions welcome! Please:
1. Fork the repository
2. Create a feature branch
3. Add tests for new features
4. Submit a pull request

## License

MIT License - see LICENSE file for details

## Contact

For questions or issues, please open a GitHub issue.

---

**Status**: âœ… Core framework implemented | ðŸ”„ Training in progress | ðŸ“Š Evaluation pending
