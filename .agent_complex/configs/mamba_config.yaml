# Mamba (SSM) Configuration

model:
  encoder_type: "mamba"
  hidden_dim: 768
  num_layers: 12
  dropout: 0.1
  
encoder:
  atom_features:
    - atomic_num
    - formal_charge
    - chirality
    - hybridization
    - num_hs
    - is_aromatic
  
  mamba:
    d_model: 768
    d_state: 16
    d_conv: 4
    expand: 2
    dt_rank: "auto"
    dt_min: 0.001
    dt_max: 0.1
    dt_init: "random"
    dt_scale: 1.0
    dt_init_floor: 1e-4
    
  linearization:
    strategy: "dfs"  # or "bfs"
    add_backtrack_tokens: true
    position_encoding: true

prediction_heads:
  rc_type:
    hidden_dims: [768, 384, 2]
  atom_center:
    use_attention: true
  bond_center:
    use_attention: true
  action:
    atom_action_vocab_size: 50
    bond_action_vocab_size: 20
  termination:
    hidden_dims: [768, 384, 2]

training:
  batch_size: 16  # Smaller due to larger model
  learning_rate: 0.00005
  weight_decay: 0.00001
  max_epochs: 100
  warmup_epochs: 5
  gradient_clip: 1.0
  
  damt_loss:
    queue_size: 50
    tau: 1.0
    initial_weights: [1.0, 1.0, 1.0, 1.0]
  
  optimizer: "adamw"
  scheduler: "cosine"
  
  early_stopping:
    patience: 10
    metric: "val_accuracy"
    mode: "max"

data:
  dataset: "uspto_50k"
  train_split: 0.8
  val_split: 0.1
  test_split: 0.1
  max_atoms: 200
  max_reactions_centers: 7
  augmentation:
    atom_mapping_randomization: true
    rc_permutation: true
    max_permutations: 24

inference:
  beam_width: 10
  max_steps: 5
  temperature: 1.0

logging:
  log_dir: "logs"
  checkpoint_dir: "checkpoints"
  save_every: 5
  log_every: 10
  use_wandb: true
  project_name: "hierretro_mamba"
